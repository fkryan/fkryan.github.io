<!DOCTYPE html>
<html>
   <head>
      <title>Fiona Ryan's Homepage</title>
      <link rel="stylesheet" type="text/css" href="./css/main.css">
      <script src="https://kit.fontawesome.com/99e4c7500b.js" crossorigin="anonymous"></script>
   </head>
   <body>
      <div class="centered">
      <header style="text-align:center">
         <h1>Fiona Ryan</h1>
      </header>
         <img src="./images/DSC_0568 2.jpeg" alt="Fiona sitting on steps" style="width:230px;border-radius:50%">
         <br>
         <container style="text-align:center;justify-content: center;">
            <a href="mailto:fkryan@gatech.edu"><i class="fa fa-envelope fa-lg" aria-hidden="true"></i></a>
            <a href="https://www.linkedin.com/in/fkryan"><i class="fa fa-linkedin fa-lg" aria-hidden="true"></i></a>
            <a href="https://twitter.com/fionakryan"><i class="fa-brands fa-twitter fa-lg"></i></a>
            <a href="https://scholar.google.com/citations?user=jAZYp8gAAAAJ&hl=en"><i class="fa-solid fa-graduation-cap fa-lg"></i></a>
         </container>
      </div>
         <div class="container">
            <div class="blurb">
               <h2>Hi, I'm Fiona.</h2>
               <p>I am a Computer Science PhD student at the Georgia Institute of Technology, where I am advised by <a href="https://rehg.org/">James Rehg</a> and <a href="https://faculty.cc.gatech.edu/~judy/">Judy Hoffman</a> within the School of Interactive Computing. 
                  My research interests are in computer vision &amp; perception for understanding human behavior. I am excited about developing video
                  and multimodal recognition models that understand people and the way they interact with each other and the world around them. I am grateful to be supported by an NSF Graduate Research Fellowship.
               </p>
               <p>I graduated from Indiana University in 2020 with a B.S. in Computer Science and a B.M. in Music Performance. 
                  At IU, I did research in the Music Informatics Lab and Computer Vision Lab and developed an interest using AI for modeling naturalistic behaviors - from practicing an instrument (and making mistakes), to interacting and conversing with other people.
                  Currently, I'm a research intern at Adobe working on multimodal video understanding. Previously, I interned at Meta working on multimodal models for conversation behavior, and Google (x2) working on medical imaging and scene understanding for Nest Cams.
               </p>
            </div>
            <!-- /.blurb -->
         </div>
         <!-- /.container -->
      <div class="container" id="news">
      	<div class="blurb">
         <h2>News</h2>
         <p><b>May 2024</b> - Interning at Adobe Research this summer! ðŸŒ‰ ðŸŽ¬
         <p><b>April 2024</b> - <a href="https://arxiv.org/abs/2403.02090"> Modeling Multimodal Social Interactions</a> and <a href="https://ego-exo4d-data.org/"> Ego-Exo4D</a> were accepted to CVPR 2024 for oral presentation!</p>	
         <p><b>December 2023</b> - Check out  <a href="https://ego-exo4d-data.org/"> Ego-Exo4D</a>, a multi-institution dataset effort that captures simultaneous egocentric and exocentric views of human activities!</p>	
         <p><b>February 2023</b> - Our paper Egocentric Auditory Attention Localization in Conversations was accepted to CVPR 2023! Check out the <a href="https://fkryan.github.io/saal"> project page</a>.</p>	
         <p><b>May 2022</b> - Excited to be interning at Meta Reality Labs Research this summer!</p>
         <p><b>April 2022</b> - I was awarded the National Science Foundation Graduate Research Fellowship!</p> 
         <p><b>February 2022</b> - <a href="https://ego4d-data.org"> Ego4D</a> was accepted to CVPR 2022!</p> 
         <details>
            <summary style="text-align:center;font-size:1em;"><i>View all</i></summary>
            <p><b>October 2021</b> - Introducing <a href="https://ego4d-data.org"> Ego4D</a>, a massive, international egocentric dataset and benchmark effort! Glad to have been a part of the social benchmark team and to have led the data collection effort at Georgia Tech.</p> 
            <p><b>July 2021</b> - Our paper <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Azizi_Big_Self-Supervised_Models_Advance_Medical_Image_Classification_ICCV_2021_paper.pdf"> Big Self-Supervised Models Advance Medical Image Classifications </a> was accepted to ICCV 2021!</p>
            <p><b>August 2020</b> - I started my PhD in Computer Science at Georgia Tech</p>
            <p><b>May 2020</b> - I am interning at Google this summer on the Dermatology team</p>
            <p><b>May 2020</b> - I graduated from Indiana University with my B.S. in Computer Science and B.M. in Clarinet Performance</p>
          </details>
         </div>
            <!-- /.blurb -->
         </div>
         <!-- /.container -->
            <div class="container" id="research">
      	<div class="blurb">
         <h2>Papers</h2>

         <container>
            <img src="./images/multimodal.png" style="width:150px;">
            <p><a href="https://arxiv.org/abs/2403.02090"><ab>Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations</ab></a><br>
               <authors>Sangmin Lee, Bolin Lai, <b>Fiona Ryan</b>, Bikram Boote, James M. Rehg</authors><br>
               <conf>CVPR 2024 (oral)</conf><br>
               <a href="https://arxiv.org/abs/2403.02090"><paperlinks>[paper]</paperlinks></a>
         </p>
        </container>





	<container>
               <img src="./images/egoexo.png" style="width:150px">
               <p><a href="https://ego-exo4d-data.org/"><ab>Ego-Exo4D: Understanding Skilled Human Activity
from First- and Third-Person Perspectives</ab></a><br>
                  <authors>Kristen Grauman et al. [including <b>Fiona Ryan]</b></authors><br>
                  <conf>CVPR 2024 (oral)</conf><br>
                  <a href="https://arxiv.org/abs/2311.18259"><paperlinks>[paper]</paperlinks></a>
                  <a href="https://ego-exo4d-data.org/"><paperlinks>[website]</paperlinks></a>
            </p>
           </container>

		
            <container>
               <img src="./images/saal.png" style="height:150px;">
               <p><a href="https://arxiv.org/abs/2303.16024"><ab>Egocentric Auditory Attention Localization in Conversations</ab></a><br>
                  <authors><b>Fiona Ryan</b>, Hao Jiang, Abhinav Shukla, James M. Rehg, Vamsi Krishna Ithapu</authors><br>
                  <conf>CVPR 2023</conf><br>
                  <a href="https://arxiv.org/abs/2303.16024"><paperlinks>[paper]</paperlinks></a>
                  <a href="https://fkryan.github.io/saal"><paperlinks>[website]</paperlinks></a>
            </p>
           </container>

           <container>
            <img src="./images/remedis.png" style="height:150px;">
            <p><a href="https://arxiv.org/pdf/2205.09723.pdf"><ab>Robust and Data-Efficient Generalization of Self-Supervised Machine Learning for Diagnostic Imaging</ab></a><br>
               <authors>Shekoofeh Azizi et al. [including <b>Fiona Ryan</b>]</authors><br>
               <conf>Nature Biomedical Engineering 2023</conf><br>
               <a href="https://www.nature.com/articles/s41551-023-01049-7"><paperlinks>[paper]</paperlinks></a>
               </p>
           </container>

           <container>
            <img src="./images/werewolf.png" style="height:150px;">
            <p><a href="https://aps.arxiv.org/abs/2212.08279"><ab>Werewolf Among Us: A Multimodal Dataset for Modeling Persuasion Behaviors in Social Deduction Games</ab></a><br>
               <authors>Bolin Lai*, Hongxin Zhang*, Miao Liu*, Aryan Pariani*, <b>Fiona Ryan</b>, Wenqi Jia, Shirley Anugrah Hayati, James M. Rehg, Diyi Yang</authors><br>
               <conf>ACL Findings 2023</conf><br>
               <a href="https://aps.arxiv.org/abs/2212.08279"><paperlinks>[paper]</paperlinks></a>
               <a href="https://persuasion-deductiongame.socialai-data.org/"><paperlinks>[website]</paperlinks></a>
               </p>
           </container>

           <container>
            <img src="./images/GLC.png" style="height:150px;">
               <p><a href="https://arxiv.org/abs/2208.04464"><ab>In the Eye of Transformer: Global-Local Correlation for Egocentric Gaze Estimation</ab></a><br>
                  <authors>Bolin Lai, Miao Liu, <b>Fiona Ryan</b>, James M Rehg</authors><br>
                  <conf>BMVC 2022 (spotlight, Best Student Paper Award)</conf><br>
                  <a href="https://arxiv.org/abs/2208.04464"><paperlinks>[paper]</paperlinks></a>
                  <a href="https://bolinlai.github.io/GLC-EgoGazeEst/"><paperlinks>[website]</paperlinks></a>
                  <a href="https://github.com/BolinLai/GLC"><paperlinks>[code]</paperlinks></a>
               </p>
            </container>
            
            <container>
               <img src="./images/ego4d.png" style="height:150px;">
               <p><a href="https://arxiv.org/abs/2110.07058"><ab>Ego4D: Around the World in 3,000 Hours of Egocentric Video</ab></a><br>
                  <authors>Kristen Grauman et al. [including <b>Fiona Ryan*</b>]</authors><br>
                  <conf>CVPR 2022 (Best Paper Award Finalist)</conf><br>
                  <a href="https://arxiv.org/abs/2110.07058"><paperlinks>[paper]</paperlinks></a>
                  <a href="https://ego4d-data.org/"><paperlinks>[website]</paperlinks></a>
               </p>
            </container>
                
            <container>
               <img src="./images/derm.png" style="height:150px;">
               <p><a href="https://arxiv.org/abs/2101.05224"><ab>Big Self-Supervised Models Advance Medical Image Classification</ab></a><br>
                  <authors>Shekoofeh Azizi, Basil Mustafa, <b>Fiona Ryan</b>, Zachary Beaver, Jan Freyberg, Jonathan Deaton, Aaron Loh, Alan Karthikesalingam, Simon Kornblith, Ting Chen, Vivek Natarajan, Mohammad Norouzi</authors><br>
                  <conf>ICCV 2021</conf><br>
                  <a href="https://arxiv.org/abs/2101.05224"><paperlinks>[paper]</paperlinks></a>
               </p>
            </container>
              
            <container>
               <img src="./images/offline.png" style="height:150px;">
               <p><a href="http://smc2019.uma.es/articles/S6/S6_02_SMC2019_paper.pdf"><ab>Offline Score Alignment For Realistic Music Practice</ab></a><br>
                  <authors>Yucong Jiang, <b>Fiona Ryan</b>, David Cartledge, Christopher Raphael</authors><br>
                  <conf>Sound and Music Computing Conference (SMC) 2019</conf><br>
                  <a href="http://smc2019.uma.es/articles/S6/S6_02_SMC2019_paper.pdf"><paperlinks>[paper]</paperlinks></a>
               </p>
            </container>
            
         </div>
            <!-- /.blurb -->
         </div>
         <!-- /.container -->
   </body>
</html>


