<!DOCTYPE html>
<html>
   <head>
      <title>Fiona Ryan's Homepage</title>
      <link rel="stylesheet" type="text/css" href="./css/main.css">
      <script src="https://kit.fontawesome.com/99e4c7500b.js" crossorigin="anonymous"></script>
   </head>
   <body>
   <div class="page-content">
      <button id="cursorToggle" style="
         position: fixed;
         top: 16px;
         right: 16px;
         display: flex;
         align-items: center;
         gap: 6px;
         background-color: #f7e1ff;
         border: 1.5px solid #dcbae8;
         border-radius: 8px;
         padding: 4px 8px;
         font-size: 12px;
         font-family: 'Arial', sans-serif;
         font-weight: bold;
         color: #5a3d5c;
         cursor: pointer;
         z-index: 10000;
         ">
         <img src="../images/tort.png" alt="cat" style="width: 18px;" />
         mode
      </button>
      <div class="centered">
      <header style="text-align:center">
         <h1>Fiona Ryan</h1>
      </header>
         <img src="./images/DSC_0568 2.jpeg" alt="Fiona sitting on steps" style="width:230px;border-radius:50%">
         <br>
         <container style="text-align:center;justify-content: center;">
            <a href="mailto:fkryan@gatech.edu"><i class="fa fa-envelope fa-lg" aria-hidden="true"></i></a>
            <a href="https://www.linkedin.com/in/fkryan"><i class="fa fa-linkedin fa-lg" aria-hidden="true"></i></a>
            <a href="https://twitter.com/fionakryan"><i class="fa-brands fa-twitter fa-lg"></i></a>
            <a href="https://scholar.google.com/citations?user=jAZYp8gAAAAJ&hl=en"><i class="fa-solid fa-graduation-cap fa-lg"></i></a>
         </container>
      </div>
         <div class="container">
            <div class="blurb">
               <h2>Hi, I'm Fiona.</h2>
               <p>I am a Computer Science PhD candidate at the Georgia Institute of Technology, where I am advised by <a href="https://rehg.org/">James Rehg</a> and <a href="https://faculty.cc.gatech.edu/~judy/">Judy Hoffman</a> within the School of Interactive Computing. 
                  My research interests are in modeling human behavior with computer vision. I am excited about developing visual and multimodal models that understand people and the way they interact with each other and the world around them. I am grateful to be supported by an NSF Graduate Research Fellowship.
               </p>
               <p>Previously, I graduated from Indiana University with a B.S. in Computer Science and a B.M. in Music Performance.<a href="#music-footnote"><i class="fa fa-music" style="font-size: 0.8em; vertical-align: top;"></i></a> 
                  At IU, I did research in the Music Informatics Lab and Computer Vision Lab and developed an interest in using AI for modeling naturalistic behaviors - from practicing an instrument (and making mistakes), to interacting and conversing with other people.
                  I have also done internships at Meta (audiovisual conversation modeling), Google (medical imaging, scene understanding), and Adobe (personalized vision-language retrieval).
               </p>

               <p><b>âœ¨ I plan to graduate in Spring 2026 and am looking for postdoc / job opportunities! âœ¨</b></p>
            </div>
            <!-- /.blurb -->
         </div>
         <!-- /.container -->
      <div class="container" id="news">
      	<div class="blurb">
         <h2>News</h2>
         <p><b>May 2025</b> - Interning at Meta this summer!
         <p><b>February 2025</b> - 3 papers accepted to CVPR 2025: <a href="https://arxiv.org/abs/2412.09586">Gaze-LLE</a>, Improving Personalized Search with Regularized Low-Rank Parameter Updates (from my Adobe internship), and SocialGesture (led by my labmate <a href="https://www.irohxucao.com/">Xu Cao</a>)!</p> 
         <p><b>December 2024</b> - Check out our new work <a href="https://arxiv.org/abs/2412.09586">Gaze-LLE</a>, a streamlined model for gaze estimation that uses the power of visual foundation models ðŸ‘€
         <p><b>July 2024</b> - Our <a href="https://arxiv.org/abs/2305.03907"> paper on audiovisual gaze forecasting</a>, led by my labmate <a href="https://bolinlai.github.io/">Bolin Lai</a>, was accepted to ECCV 2024!
         <p><b>May 2024</b> - Interning at Adobe Research this summer! ðŸŒ‰ ðŸŽ¬
         <p><b>April 2024</b> - <a href="https://arxiv.org/abs/2403.02090"> Modeling Multimodal Social Interactions</a> (led by my labmate <a href="https://sites.google.com/view/sangmin-lee">Sangmin Lee</a>) and <a href="https://ego-exo4d-data.org/"> Ego-Exo4D</a> were accepted to CVPR 2024 for oral presentation!</p>	
         <p><b>December 2023</b> - Check out  <a href="https://ego-exo4d-data.org/"> Ego-Exo4D</a>, a multi-institution dataset effort that captures simultaneous egocentric and exocentric views of human activities!</p>	
         <details>
            <summary style="text-align:center;font-size:1em;"><i>View all</i></summary>
            <p><b>February 2023</b> - Our paper Egocentric Auditory Attention Localization in Conversations was accepted to CVPR 2023! Check out the <a href="https://fkryan.github.io/saal"> project page</a>.</p>	
            <p><b>May 2022</b> - Excited to be interning at Meta Reality Labs Research this summer!</p>
            <p><b>April 2022</b> - I was awarded the National Science Foundation Graduate Research Fellowship!</p> 
            <p><b>February 2022</b> - <a href="https://ego4d-data.org"> Ego4D</a> was accepted to CVPR 2022!</p> 
            <p><b>October 2021</b> - Introducing <a href="https://ego4d-data.org"> Ego4D</a>, a massive, international egocentric dataset and benchmark effort! Glad to have been a part of the social benchmark team and to have led the data collection effort at Georgia Tech.</p> 
            <p><b>July 2021</b> - Our paper <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Azizi_Big_Self-Supervised_Models_Advance_Medical_Image_Classification_ICCV_2021_paper.pdf"> Big Self-Supervised Models Advance Medical Image Classifications </a> was accepted to ICCV 2021!</p>
            <p><b>August 2020</b> - I started my PhD in Computer Science at Georgia Tech</p>
            <p><b>May 2020</b> - I am interning at Google this summer on the Dermatology team</p>
            <p><b>May 2020</b> - I graduated from Indiana University with my B.S. in Computer Science and B.M. in Clarinet Performance</p>
          </details>
         </div>
            <!-- /.blurb -->
         </div>
         <!-- /.container -->
            <div class="container" id="research">
      	<div class="blurb">
         <h2>Papers</h2>
         <container>
            <img src="./images/gazelle.png" style="width:150px;">
            <p><a href="https://arxiv.org/abs/2412.09586"><ab>Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders</ab></a><br>
               <authors><b>Fiona Ryan</b>, Ajay Bati, Sangmin Lee, Daniel Bolya, Judy Hoffman*, James M. Rehg*</authors><br>
               <conf>CVPR 2025 <confnote>(Highlight)</confnote></conf><br>
               <a href="https://arxiv.org/abs/2412.09586"><paperlinks>[paper]</paperlinks></a>
               <a href="https://github.com/fkryan/gazelle"><paperlinks>[code]</paperlinks></a>
         </p>
        </container>
        <container>
         <img src="./images/POLAR.png" style="width:150px;">
         <p><a href="https://arxiv.org/abs/2506.10182"><ab>Improving Personalized Search with Regularized Low-Rank Parameter Updates</ab></a><br>
            <authors><b>Fiona Ryan</b>, Josef Sivic, Fabian Caba Heilbron, Judy Hoffman, James M. Rehg, Bryan Russell</authors><br>
            <conf>CVPR 2025 <confnote>(Highlight)</confnote></conf><br>
            <a href="https://arxiv.org/abs/2506.10182"><paperlinks>[paper]</paperlinks></a>
            <a href="https://github.com/adobe-research/polar-vl"><paperlinks>[code]</paperlinks></a>
            </p>
         </container>
         <container>
            <img src="./images/socialgesture.png" style="width:150px;">
            <p><a href="https://arxiv.org/abs/2504.02244"><ab>SocialGesture: Delving into Multi-person Gesture Understanding</ab></a><br>
               <authors>Xu Cao, Pranav Virupaksha, Wenqi Jia, Bolin Lai, <b>Fiona Ryan</b>, Sangmin Lee, James M. Rehg</authors><br>
               <conf>CVPR 2025</conf><br>
               <a href="https://arxiv.org/abs/2504.02244"><paperlinks>[paper]</paperlinks></a><a href="https://huggingface.co/datasets/IrohXu/SocialGesture"><paperlinks>[data]</paperlinks></a>
            </p>
         </container>
         <container>
            <img src="./images/social_survey.png" style="width:150px;">
            <p><a href="https://arxiv.org/abs/2409.15316"><ab>Towards Social AI: A Survey on Understanding Social Interactions</ab></a><br>
               <authors>Sangmin Lee, Minzhi Li, Bolin Lai, Wenqi Jia, <b>Fiona Ryan</b>, Xu Cao, Ozgur Kara, Bikram Boote, Weiyan Shi, Diyi Yang, James M. Rehg</authors><br>
               <conf>arXiv preprint</conf><br>
               <a href="https://arxiv.org/abs/2409.15316"><paperlinks>[paper]</paperlinks></a>
         </p>
        </container>
         <container>
            <img src="./images/csts.png" style="width:150px;">
            <p><a href="https://arxiv.org/abs/2305.03907"><ab>Listen to Look into the Future: Audio-Visual Egocentric Gaze Anticipation</ab></a><br>
               <authors>Bolin Lai, <b>Fiona Ryan</b>, Wenqi Jia, Miao Liu*, James M. Rehg*</authors><br>
               <conf>ECCV 2024</conf><br>
               <a href="https://arxiv.org/abs/2305.03907"><paperlinks>[paper]</paperlinks></a>
               <a href="https://bolinlai.github.io/CSTS-EgoGazeAnticipation/"><paperlinks>[website]</paperlinks></a>
               <a href="https://github.com/BolinLai/CSTS"><paperlinks>[code]</paperlinks></a>
         </p>
        </container>
         <container>
            <img src="./images/multimodal.png" style="width:150px;">
            <p><a href="https://arxiv.org/abs/2403.02090"><ab>Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations</ab></a><br>
               <authors>Sangmin Lee, Bolin Lai, <b>Fiona Ryan</b>, Bikram Boote, James M. Rehg</authors><br>
               <conf>CVPR 2024 <confnote>(Oral)</confnote></conf><br>
               <a href="https://arxiv.org/abs/2403.02090"><paperlinks>[paper]</paperlinks></a>
               <a href="https://sangmin-git.github.io/projects/MMSI/"><paperlinks>[website]</paperlinks></a>
               <a href="https://github.com/sangmin-git/MMSI"><paperlinks>[code]</paperlinks></a>
               <a href="https://www.dropbox.com/scl/fo/fbv6njzu1ynbgv9wgtrwo/ANPk2TKqK2rl44MqKu05ogk?rlkey=yx7bmzmmiymauvz99q2rvjajg&e=1&st=305631zj&dl=0"><paperlinks>[data]</paperlinks></a>
         </p>
        </container>
	<container>
               <img src="./images/egoexo.png" style="width:150px">
               <p><a href="https://ego-exo4d-data.org/"><ab>Ego-Exo4D: Understanding Skilled Human Activity
from First- and Third-Person Perspectives</ab></a><br>
                  <authors>Kristen Grauman et al. [including <b>Fiona Ryan</b>]</authors><br>
                  <conf>CVPR 2024 <confnote>(Oral)</confnote></conf><br>
                  <a href="https://arxiv.org/abs/2311.18259"><paperlinks>[paper]</paperlinks></a>
                  <a href="https://ego-exo4d-data.org/"><paperlinks>[website]</paperlinks></a>
            </p>
           </container>

		
            <container>
               <img src="./images/saal.png" style="height:150px;">
               <p><a href="https://arxiv.org/abs/2303.16024"><ab>Egocentric Auditory Attention Localization in Conversations</ab></a><br>
                  <authors><b>Fiona Ryan</b>, Hao Jiang, Abhinav Shukla, James M. Rehg, Vamsi Krishna Ithapu</authors><br>
                  <conf>CVPR 2023</conf><br>
                  <a href="https://arxiv.org/abs/2303.16024"><paperlinks>[paper]</paperlinks></a>
                  <a href="https://fkryan.github.io/saal"><paperlinks>[website]</paperlinks></a>
            </p>
           </container>

           <container>
            <img src="./images/remedis.png" style="height:150px;">
            <p><a href="https://arxiv.org/pdf/2205.09723.pdf"><ab>Robust and Data-Efficient Generalization of Self-Supervised Machine Learning for Diagnostic Imaging</ab></a><br>
               <authors>Shekoofeh Azizi et al. [including <b>Fiona Ryan</b>]</authors><br>
               <conf>Nature Biomedical Engineering 2023</conf><br>
               <a href="https://www.nature.com/articles/s41551-023-01049-7"><paperlinks>[paper]</paperlinks></a>
               </p>
           </container>

           <container>
            <img src="./images/werewolf.png" style="height:150px;">
            <p><a href="https://aps.arxiv.org/abs/2212.08279"><ab>Werewolf Among Us: A Multimodal Dataset for Modeling Persuasion Behaviors in Social Deduction Games</ab></a><br>
               <authors>Bolin Lai*, Hongxin Zhang*, Miao Liu*, Aryan Pariani*, <b>Fiona Ryan</b>, Wenqi Jia, Shirley Anugrah Hayati, James M. Rehg, Diyi Yang</authors><br>
               <conf>ACL Findings 2023</conf><br>
               <a href="https://aps.arxiv.org/abs/2212.08279"><paperlinks>[paper]</paperlinks></a>
               <a href="https://persuasion-deductiongame.socialai-data.org/"><paperlinks>[website]</paperlinks></a>
               </p>
           </container>

           <container>
            <img src="./images/GLC.png" style="height:150px;">
               <p><a href="https://arxiv.org/abs/2208.04464"><ab>In the Eye of Transformer: Global-Local Correlation for Egocentric Gaze Estimation</ab></a><br>
                  <authors>Bolin Lai, Miao Liu, <b>Fiona Ryan</b>, James M Rehg</authors><br>
                  <conf>BMVC 2022 <confnote>(Spotlight, Best Student Paper Award)</confnote></conf><br>
                  <a href="https://arxiv.org/abs/2208.04464"><paperlinks>[paper]</paperlinks></a>
                  <a href="https://bolinlai.github.io/GLC-EgoGazeEst/"><paperlinks>[website]</paperlinks></a>
                  <a href="https://github.com/BolinLai/GLC"><paperlinks>[code]</paperlinks></a>
               </p>
            </container>
            
            <container>
               <img src="./images/ego4d.png" style="height:150px;">
               <p><a href="https://arxiv.org/abs/2110.07058"><ab>Ego4D: Around the World in 3,000 Hours of Egocentric Video</ab></a><br>
                  <authors>Kristen Grauman et al. [including <b>Fiona Ryan*</b>]</authors><br>
                  <conf>CVPR 2022 <confnote>(Oral, Best Paper Award Finalist)</confnote></conf><br>
                  <a href="https://arxiv.org/abs/2110.07058"><paperlinks>[paper]</paperlinks></a>
                  <a href="https://ego4d-data.org/"><paperlinks>[website]</paperlinks></a>
               </p>
            </container>
                
            <container>
               <img src="./images/derm.png" style="height:150px;">
               <p><a href="https://arxiv.org/abs/2101.05224"><ab>Big Self-Supervised Models Advance Medical Image Classification</ab></a><br>
                  <authors>Shekoofeh Azizi, Basil Mustafa, <b>Fiona Ryan</b>, Zachary Beaver, Jan Freyberg, Jonathan Deaton, Aaron Loh, Alan Karthikesalingam, Simon Kornblith, Ting Chen, Vivek Natarajan, Mohammad Norouzi</authors><br>
                  <conf>ICCV 2021</conf><br>
                  <a href="https://arxiv.org/abs/2101.05224"><paperlinks>[paper]</paperlinks></a>
               </p>
            </container>
              
            <container>
               <img src="./images/offline.png" style="height:150px;">
               <p><a href="http://smc2019.uma.es/articles/S6/S6_02_SMC2019_paper.pdf"><ab>Offline Score Alignment For Realistic Music Practice</ab></a><br>
                  <authors>Yucong Jiang, <b>Fiona Ryan</b>, David Cartledge, Christopher Raphael</authors><br>
                  <conf>Sound and Music Computing Conference (SMC) 2019</conf><br>
                  <a href="http://smc2019.uma.es/articles/S6/S6_02_SMC2019_paper.pdf"><paperlinks>[paper]</paperlinks></a>
               </p>
            </container>
            
         </div>
            <!-- /.blurb -->
         </div>
         <!-- /.container -->
         <div id="music-footnote" style="margin-top: 50px; text-align: center;">
            <p><i class="fa fa-music" style="font-size: 0.8em; vertical-align: top;"></i>  Click if you would like to listen to undergrad-era me playing clarinet while you peruse this website.</p>
            <audio controls>
               <source src="assets/messager.wav" type="audio/wav">
               Your browser does not support the audio element.
            </audio>
         </div>
         <script src="cursor.js"></script>
      </div>
   </body>
</html>


