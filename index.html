<!DOCTYPE html>
<html>
   <head>
      <title>Fiona Ryan's Homepage</title>
      <link rel="stylesheet" type="text/css" href="./css/main.css">
   </head>
   <body>
   <div class="centered">
      <header>
         <h1>Fiona Ryan</h1>
      </header>
         <img src="./images/DSC_0568 2.jpeg" alt="Girl sitting on steps" style="width:230px;border-radius:50%">
         <ul>
            <li><a href="mailto:fkryan@gatech.edu"><smalltype>[email]</smalltype></a></li>
            <li><a href="https://www.linkedin.com/in/fkryan"><smalltype>[linkedin]</smalltype></a></li>
            <li><a href="https://twitter.com/fionakryan"><smalltype>[twitter]</smalltype></a></li>
         </ul>
         </div>
         <div class="container">
            <div class="blurb">
               <h2>Hi, I'm Fiona.</h2>
               <p>I am a Computer Science PhD student at the Georgia Institute of Technology, where I am advised by Professor <a href="https://rehg.org/">James Rehg</a> within the School of Interactive Computing. 
                  My research interests are in computer vision &amp; perception, with applications to understanding human behavior and behavioral health indicators. I am excited about developing video
                  and multimodal recognition models that understand people and the way they interact with each other and the world around them. I am grateful to be supported by an NSF Graduate Research Fellowship.
               </p>
               <p>I graduated from Indiana University in 2020 with a B.S. in Computer Science and a B.M. in Clarinet Performance. 
                  At IU, I did research in the Music Informatics Lab and Computer Vision Lab and developed an interest using AI for modeling naturalistic behaviors - from practicing an instrument (and making mistakes), to interacting and conversing with other people.
                  I have also spent 2 summers interning at Google, where I worked on deep learning for diagnosing skin conditions and scene understanding for Nest Cams.
		  Most recently I interned at Meta Reality Labs on the Audio Team, where I worked on multimodal models for conversation behavior.
               </p>
            </div>
            <!-- /.blurb -->
         </div>
         <!-- /.container -->
      <div class="container" id="news">
      	<div class="blurb">
         <h2>News</h2>
         <p><b>February 2023</b> - Our paper Egocentric Auditory Attention Localization in Conversations was accepted to CVPR 2023! Check out the <a href="https://fkryan.github.io/saal"> project page</a>.</p>	
         <p><b>January 2023</b> - I am a Teaching Assistant for Intro to Computer Vision this semester. Looking forward to working with students!</p>
         <p><b>May 2022</b> - Excited to be interning at Meta Reality Labs Research this summer!</p>
         <p><b>April 2022</b> - I was awarded the National Science Foundation Graduate Research Fellowship!</p> 
         <p><b>February 2022</b> - <a href="https://ego4d-data.org"> Ego4D</a> was accepted to CVPR 2022!</p> 
         <p><b>October 2021</b> - Introducing <a href="https://ego4d-data.org"> Ego4D</a>, a massive, international egocentric dataset and benchmark effort! Glad to have been a part of the social benchmark team and to have led the data collection effort at Georgia Tech.</p> 
         <p><b>July 2021</b> - Our paper <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Azizi_Big_Self-Supervised_Models_Advance_Medical_Image_Classification_ICCV_2021_paper.pdf"> Big Self-Supervised Models Advance Medical Image Classifications </a> was accepted to ICCV 2021!</p>
         <details>
            <summary><i>View all</i></summary>
            <p><b>August 2020</b> - I started my PhD in Computer Science at Georgia Tech</p>
            <p><b>May 2020</b> - I am interning at Google this summer on the Dermatology team</p>
            <p><b>May 2020</b> - I graduated from Indiana University with my B.S. in Computer Science and B.M. in Clarinet Performance</p>
          </details>
         </div>
            <!-- /.blurb -->
         </div>
         <!-- /.container -->
            <div class="container" id="research">
      	<div class="blurb">
         <h2>Papers</h2>

            <container>
               <img src="./images/saal.png" style="height:225px;">
               <p><a href="https://arxiv.org/abs/2303.16024"><ab>Egocentric Auditory Attention Localization in Conversations</ab></a><br>
                  <authors><b>Fiona Ryan</b>, Hao Jiang, Abhinav Shukla, James M Rehg, Vamsi Krishna Ithapu</authors><br>
                  <conf>Accepted to CVPR 2023</conf><br>
                  <a href="https://arxiv.org/abs/2303.16024"><paperlinks>[paper]</paperlinks></a>
                  <a href="https://fkryan.github.io/saal"><paperlinks>[website]</paperlinks></a>
            </p>
           </container>

           <container>
            <img src="./images/glc.png" style="height:225px;">
               <p><a href="https://arxiv.org/abs/2208.04464"><ab>In the Eye of Transformer: Global-Local Correlation for Egocentric Gaze Estimation</ab></a><br>
                  <authors>Bolin Lai, Miao Liu, <b>Fiona Ryan</b>, James M Rehg</authors><br>
                  <conf>BMVC 2022 (spotlight, Best Student Paper Award)</conf><br>
                  <a href="https://arxiv.org/abs/2303.16024"><paperlinks>[paper]</paperlinks></a>
                  <a href="https://bolinlai.github.io/GLC-EgoGazeEst/"><paperlinks>[website]</paperlinks></a>
                  <a href="https://github.com/BolinLai/GLC"><paperlinks>[code]</paperlinks></a>
               </p>
            </container>
            
            <container>
               <img src="./images/ego4d.png" style="height:225px;">
               <p><a href="https://arxiv.org/abs/2110.07058"><ab>Ego4D: Around the World in 3,000 Hours of Egocentric Video</ab></a><br>
                  <authors>Kristen Grauman,...<b>Fiona Ryan*</b>,...Jitendra Malik</authors><br>
                  <conf>CVPR 2022 (oral)</conf><br>
                  <a href="https://arxiv.org/abs/2110.07058"><paperlinks>[paper]</paperlinks></a>
                  <a href="https://ego4d-data.org/"><paperlinks>[website]</paperlinks></a>
               </p>
            </container>
                
            <container>
               <img src="./images/derm.png" style="height:225px;">
               <p><a href="https://arxiv.org/abs/2101.05224"><ab>Big Self-Supervised Models Advance Medical Image Classification</ab></a><br>
                  <authors>Shekoofeh Azizi, Basil Mustafa, <b>Fiona Ryan</b>, Zachary Beaver, Jan Freyberg, Jonathan Deaton, Aaron Loh, Alan Karthikesalingam, Simon Kornblith, Ting Chen, Vivek Natarajan, Mohammad Norouzi</authors><br>
                  <conf>ICCV 2021</conf><br>
                  <a href="https://arxiv.org/abs/2101.05224"><paperlinks>[paper]</paperlinks></a>
               </p>
            </container>
              
            <container>
               <img src="./images/offline.png" style="height:225px;">
               <p><a href="http://smc2019.uma.es/articles/S6/S6_02_SMC2019_paper.pdf"><ab>Offline Score Alignment For Realistic Music Practice</ab></a><br>
                  <authors>Yucong Jiang, <b>Fiona Ryan</b>, David Cartledge, Christopher Raphael</authors><br>
                  <conf>Sound and Music Computing Conference (SMC) 2019</conf><br>
                  <a href="http://smc2019.uma.es/articles/S6/S6_02_SMC2019_paper.pdf"><paperlinks>[paper]</paperlinks></a>
               </p>
            </container>
            
         </div>
            <!-- /.blurb -->
         </div>
         <!-- /.container -->
   </body>
</html>


