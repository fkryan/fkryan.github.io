<html>
<head>
  <meta charset="utf-8">
  <title>Gaze-LLE</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Gaze-LLE: Simplifying Gaze Target Etimation by Leveraging an Already-Learned Encoder</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://fkryan.github.io/">Fiona Ryan</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/abati777/">Ajay Bati</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/sangmin-lee">Sangmin Lee</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://dbolya.github.io/">Daniel Bolya</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://faculty.cc.gatech.edu/~judy/">Judy Hoffman</a><sup>*1</sup>,
            </span>
            <span class="author-block">
              <a href="https://rehg.org/">James M. Rehg</a><sup>*2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Georgia Tech,</span>
            <span class="author-block"><sup>2</sup>UIUC</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered"
        <div class="figure">
            <img src="./static/images/teaser.png" width="70%">
        </div>
        <h2 class="subtitle has-text-centered">
          A simple, effective new approach for gaze target estimation leveraging the power of general-purpose visual encoders.
        </h2>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <video poster="" id="" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/CBS_This_Morning_12407_12648_3.mov"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Jamie_Oliver_725_800_0.mov"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/It's_Always_Sunny_in_Philadelphia_25230_25410_1.mov"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/CBS_This_Morning_1948_2128_0.mov"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/MLB_Interview_18341_18942_3.mov"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Titanic_4255_4376_1.mov"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/West_World_74424_74568_2.mov"
                    type="video/mp4">
          </video>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We address the problem of gaze target estimation, which aims to predict where a person is looking in a scene. Predicting a person’s gaze target requires reasoning both about the person’s appearance and the contents of the scene. Prior works have developed increasingly complex, hand-crafted pipelines for gaze target estimation that carefully fuse features from separate scene encoders, head encoders, and auxiliary models for signals like depth and pose. Motivated by the success of general-purpose feature extractors on a variety of visual tasks, we propose Gaze-LLE, a novel transformer framework that simplifies gaze target estimation by leveraging only features from a frozen DINOv2 encoder. We extract a single feature representation for the scene, and apply a person-specific positional prompt to decode gaze with a lightweight module. We demonstrate the effectiveness of our approach by achieving state-of-the-art performance across several gaze benchmarks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Architecture. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Approach</h2>
        <div class="figure">
          <img src="./static/images/arch.png" width="90%">
        </div>
      </div>
    </div>
    We propose a new architecture for gaze target estimation built upon a single <i>frozen</i> DINOv2 visual encoder. Our method learns a lightweight decoder that uses a positional <i>head prompting</i> mechanism to decode the gaze for a specific person from the DINOv2 features.
    <!--/ Architecture. -->

    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="figure">
          <img src="./static/images/results.png" width="90%">
        </div>
      </div>
    </div>
    Our method achieves SoTA results with a single encoder, no auxiliary inputs like estimated depth/pose, and significantly fewer parameters.
    <!--/ Results. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>

  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Paper and Code Coming Soon!</h2>
        <div class="content has-text-justified">
          <p>
            For questions, contact Fiona Ryan (fkryan at gatech dot edu)
          </p>
        </div>
      </div>
    </div>

</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Thanks for the template <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>!
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>